import os
from pathlib import Path
from typing import List
from dotenv import load_dotenv
from docx import Document as DocxDocument
from langchain_core.documents import Document as LangchainDocument
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from tqdm import tqdm
import fitz  # PyMuPDF
from pdf2image import convert_from_path
import pytesseract
import re

# ============================
# .env èª­ã¿è¾¼ã¿
# ============================
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

# ============================
# åˆ‡ã‚Šæ›¿ãˆå¯èƒ½ãªåŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«
# ============================
USE_OPENAI = True  # â† åˆ‡ã‚Šæ›¿ãˆå¯èƒ½

if USE_OPENAI:
    from langchain_openai import OpenAIEmbeddings
    if not OPENAI_API_KEY:
        raise ValueError("âŒ OPENAI_API_KEY ãŒ .env ã«è¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
    embedding_model = OpenAIEmbeddings(
        model="text-embedding-3-small",
        openai_api_key=OPENAI_API_KEY
    )
    VECTOR_STORE_PATH = Path("vectorstore")
    print("ğŸ§  OpenAIåŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
else:
    from langchain_community.embeddings import HuggingFaceEmbeddings
    embedding_model = HuggingFaceEmbeddings(model_name="intfloat/multilingual-e5-small")
    VECTOR_STORE_PATH = Path("vectorstore_hf")
    print("ğŸ§  HuggingFaceåŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")

# ============================
# ãƒ‘ã‚¹è¨­å®š
# ============================
DOCX_DIR = Path("data/docx")
PDF_DIR = Path("data/pdf")
TXT_DIR = Path("data/all")

# ============================
# OCRã‚ã‚Š or ãªã—ã«é–¢ã‚ã‚‰ãšPDFå‡¦ç†
# ============================
def process_and_save_with_ocr():
    documents = []

    for pdf_path in Path(PDF_DIR).glob("*.pdf"):
        print(f"ğŸ“„ PDFå‡¦ç†ä¸­: {pdf_path.name}")

        # ã‚¹ã‚¿ã‚¤ãƒ«ä»˜ãæŠ½å‡ºï¼ˆãƒãƒ„å°ã‚„èµ¤æ–‡å­—å„ªå…ˆï¼‰
        pages = extract_text_with_styles(pdf_path)

        # ãƒšãƒ¼ã‚¸ãŒç©ºãªã‚‰OCRã¸ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        if all(not p.strip() for p in pages):
            print(f"âš ï¸ ã‚¹ã‚¿ã‚¤ãƒ«æŠ½å‡ºå¤±æ•—: {pdf_path.name} â†’ OCRã«åˆ‡ã‚Šæ›¿ãˆ")
            pages = extract_text_from_scanned_pdf(pdf_path)

        # OCRã§ã‚‚ç©ºãªã‚‰ãƒ†ã‚­ã‚¹ãƒˆPDFã¨ã—ã¦å‡¦ç†
        if all(not p.strip() for p in pages):
            print(f"âš ï¸ OCRå¤±æ•—: {pdf_path.name} â†’ ãƒ†ã‚­ã‚¹ãƒˆå‹PDFã¨ã—ã¦å‡¦ç†ã—ã¾ã™")
            full_text = extract_text_from_pdf(pdf_path)
            if not full_text.strip():
                print(f"âŒ PDFãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºã‚‚å¤±æ•—: {pdf_path.name} â†’ ã‚¹ã‚­ãƒƒãƒ—")
                continue
            metadata = {
                "source": pdf_path.name,
                "id": f"{pdf_path.stem}",
                "page_number": 1,
                "section": extract_section_title(full_text),
            }
            documents.append(LangchainDocument(page_content=full_text, metadata=metadata))
        else:
            for i, page_text in enumerate(pages):
                if not page_text.strip():
                    continue
                metadata = {
                    "source": pdf_path.name,
                    "id": f"{pdf_path.stem}_p{i+1}",
                    "page_number": i + 1,
                    "section": extract_section_title(page_text),
                }
                documents.append(LangchainDocument(page_content=page_text, metadata=metadata))

    if not documents:
        print("âŒ æ–‡æ›¸ãŒ1ã¤ã‚‚ç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
        return

    print(f"ğŸ§± ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²ä¸­...ï¼ˆæ–‡æ›¸æ•°: {len(documents)}ï¼‰")
    splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=512, chunk_overlap=128)
    split_docs = splitter.split_documents(documents)
    print(f"âœ… åˆ†å‰²å¾Œãƒãƒ£ãƒ³ã‚¯æ•°: {len(split_docs)}")

    save_vector_store(split_docs, VECTOR_STORE_PATH)


# ============================
# ã‚¹ã‚¿ã‚¤ãƒ«ã‚’ä¿æŒã—ãªãŒã‚‰OCR
# ============================
def extract_text_with_styles(pdf_path: Path) -> List[str]:
    """PDFãƒšãƒ¼ã‚¸ã”ã¨ã«ã€è‰²ã‚„è£…é£¾ã‚’åˆ¤åˆ¥ã—ã¤ã¤ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º"""
    doc = fitz.open(str(pdf_path))
    processed_pages = []

    for page in doc:
        page_text = ""
        blocks = page.get_text("dict")["blocks"]
        for b in blocks:
            if "lines" not in b:
                continue
            for line in b["lines"]:
                for span in line["spans"]:
                    text = span["text"].strip()
                    if not text:
                        continue

                    # åˆ¤åˆ¥æ¡ä»¶
                    color = span.get("color", 0)
                    is_strike = span.get("flags", 0) & 8 != 0  # 8 = strike-through

                    # âŒ æ‰“ã¡æ¶ˆã—ç·šã®æ–‡å­—ã¯ç„¡è¦–
                    if is_strike:
                        continue

                    # âœ… è‰²ä»˜ãæ–‡å­—ï¼ˆèµ¤ãƒ»ç·‘ï¼‰ã®ã¿æŠ½å‡ºï¼ˆä¾‹ï¼šRGBå€¤ï¼‰
                    if color in [0xFF0000, 0x00FF00]:  # èµ¤ã¾ãŸã¯ç·‘
                        page_text += f"{text} "

                    # âšª é€šå¸¸æ–‡å­—ã‚‚æŠ½å‡ºï¼ˆå„ªå…ˆåº¦ä½ï¼‰
                    elif color == 0x000000:
                        page_text += f"{text} "

        processed_pages.append(page_text.strip())

    return processed_pages


# ============================
# ãƒ†ã‚­ã‚¹ãƒˆå‹PDFæŠ½å‡º
# ============================
def extract_text_from_pdf(pdf_path: Path) -> str:
    try:
        doc = fitz.open(pdf_path)
        return "\n".join(page.get_text() for page in doc)
    except Exception as e:
        print(f"âŒ PDFèª­ã¿å–ã‚Šå¤±æ•—: {pdf_path.name} - {e}")
        return ""

# ============================
# OCRã§ãƒšãƒ¼ã‚¸ã”ã¨ã«æŠ½å‡º
# ============================
def extract_text_from_scanned_pdf(pdf_path: Path) -> List[str]:
    try:
        images = convert_from_path(str(pdf_path))
        return [pytesseract.image_to_string(img, lang="jpn") for img in images]
    except Exception as e:
        print(f"âŒ OCRå¤±æ•—: {pdf_path.name} - {e}")
        return []

# ============================
# ç« ã‚¿ã‚¤ãƒˆãƒ«æŠ½å‡º
# ============================
def extract_section_title(text: str) -> str:
    """ç« ã‚¿ã‚¤ãƒˆãƒ«ã‚‰ã—ãã‚‚ã®ã‚’æŠ½å‡º"""
    patterns = [
        r"(ç¬¬[0-9ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+ç« \s*.+)",
        r"^\s*[0-9]+\.\s+.+",  # 1. ã¯ã˜ã‚ã«
    ]
    for pattern in patterns:
        match = re.search(pattern, text, re.MULTILINE)
        if match:
            try:
                # ã‚°ãƒ«ãƒ¼ãƒ—ãŒå­˜åœ¨ã™ã‚‹å ´åˆã®ã¿
                return match.group(1).strip()
            except IndexError:
                return match.group(0).strip()
    return "ä¸æ˜"


# ============================
# FAISSãƒ™ã‚¯ãƒˆãƒ«ä¿å­˜
# ============================
def save_vector_store(docs: List[LangchainDocument], output_dir: Path, batch_size: int = 50):
    if not docs:
        print("âš ï¸ æ–‡æ›¸ãŒç©ºã§ã™")
        return

    texts = [doc.page_content for doc in docs]
    metadatas = [doc.metadata for doc in docs]

    print(f"ğŸ§  åŸ‹ã‚è¾¼ã¿é–‹å§‹: {len(texts)}ä»¶")
    embeddings = []
    for i in tqdm(range(0, len(texts), batch_size), desc="ğŸ”„ ãƒ™ã‚¯ãƒˆãƒ«åŒ–ä¸­"):
        batch = texts[i:i + batch_size]
        try:
            batch_vectors = embedding_model.embed_documents(batch)
            embeddings.extend(batch_vectors)
        except Exception as e:
            print(f"âŒ åŸ‹ã‚è¾¼ã¿å¤±æ•—ï¼ˆ{i}ä»¶ç›®ï¼‰: {e}")

    if not embeddings:
        print("âŒ ãƒ™ã‚¯ãƒˆãƒ«ãŒç”Ÿæˆã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚")
        return

    try:
        faiss_db = FAISS.from_embeddings(
            list(zip(texts, embeddings)),
            embedding=embedding_model,
            metadatas=metadatas
        )
        output_dir.mkdir(parents=True, exist_ok=True)
        faiss_db.save_local(str(output_dir))
        print(f"âœ… ãƒ™ã‚¯ãƒˆãƒ«ä¿å­˜å®Œäº†: {output_dir.resolve()}")
    except Exception as e:
        print(f"âŒ FAISS ä¿å­˜å¤±æ•—: {e}")

# ============================
# å®Ÿè¡Œãƒ•ãƒ­ãƒ¼
# ============================
if __name__ == "__main__":
    process_and_save_with_ocr()
