import os
import logging
from typing import Union, List, Dict, Any
from pathlib import Path
import traceback
import torch
from dotenv import load_dotenv

from langchain.chains import RetrievalQA
from langchain_core.documents import Document
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
try:
    import streamlit as st
    secrets = st.secrets
except ModuleNotFoundError:
    secrets = {}


# ========== è¨­å®šèª­ã¿è¾¼ã¿ ========== #
load_dotenv()

def get_secret(key, default=None):
    if "st" in globals() and hasattr(st, "secrets"):
        return os.getenv(key) or st.secrets.get(key, default)
    return os.getenv(key, default)


OPENAI_API_KEY = get_secret("OPENAI_API_KEY")
base_dir = Path(__file__).resolve().parents[2]  # frontend â†’ src â†’ project root
def get_vector_store_path(use_openai: bool) -> Path:
    """use_openaiã«å¿œã˜ã¦ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã®ãƒ‘ã‚¹ã‚’åˆ‡ã‚Šæ›¿ãˆã‚‹"""
    store_dir_name = "vectorstore" if use_openai else "vectorstore_hf"
    return base_dir / store_dir_name


# ========== ãƒ­ã‚°è¨­å®š ========== #
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s"
)

# ========== ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹ç¢ºèª ========== #
device = "cuda" if torch.cuda.is_available() else "cpu"
logging.info(f"âœ… ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {device}")

# ========== ãƒ¢ãƒ‡ãƒ«å ========== #
embedding_model_name = "intfloat/multilingual-e5-small"
llm_model_name = "rinna/youri-7b-chat"

# ========== åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«é¸æŠ ========== #
def get_embedding_model(use_openai: bool):
    if use_openai:
        if not OPENAI_API_KEY:
            raise EnvironmentError("âŒ OPENAI_API_KEY ãŒæœªè¨­å®šã§ã™ã€‚")
        from langchain_openai import OpenAIEmbeddings
        logging.info("ğŸ§  OpenAI Embeddings ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
        return OpenAIEmbeddings(model="text-embedding-3-small", openai_api_key=OPENAI_API_KEY)
    else:
        from langchain_community.embeddings import HuggingFaceEmbeddings
        logging.info("ğŸ§  HuggingFace Embeddings ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
        return HuggingFaceEmbeddings(model_name=embedding_model_name)

# ========== LLM ãƒ¢ãƒ‡ãƒ«é¸æŠ ========== #
def get_llm(use_openai: bool):
    if use_openai:
        from langchain_openai import ChatOpenAI
        logging.info("ğŸ¤– OpenAI LLMï¼ˆgpt-4oï¼‰ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
        return ChatOpenAI(model="gpt-4o", temperature=0, openai_api_key=OPENAI_API_KEY)
    else:
        from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
        from langchain_huggingface import HuggingFacePipeline  # âœ… ä¿®æ­£ãƒã‚¤ãƒ³ãƒˆ

        logging.info(f"ğŸ¤– HuggingFace LLMï¼ˆ{llm_model_name}ï¼‰ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...")
        tokenizer = AutoTokenizer.from_pretrained(llm_model_name)
        model = AutoModelForCausalLM.from_pretrained(
            llm_model_name,
            device_map="auto",
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32
        )

        pipe = pipeline(
            "text-generation",
            model=model,
            tokenizer=tokenizer,
            return_full_text=False,
            max_new_tokens=512,
            do_sample=False
        )

        logging.info("âœ… HuggingFace LLM ã®ãƒ­ãƒ¼ãƒ‰å®Œäº†")
        return HuggingFacePipeline(pipeline=pipe)


# ========== ãƒã‚§ãƒ¼ãƒ³ç”Ÿæˆ ========== #
def get_retrieval_chain(llm, vectorstore: FAISS) -> RetrievalQA:
    return RetrievalQA.from_chain_type(
        llm=llm,
        retriever=vectorstore.as_retriever(search_kwargs={"k": 3}),
        return_source_documents=True
    )

# ========== RAG ãƒãƒ£ãƒƒãƒˆé–¢æ•° ========== #
def rag_chatbot(
    input_text: str,
    vector_store_path: str = None,
    use_openai: bool = False
) -> Dict[str, Union[str, List[Dict[str, Any]]]]:

    # âœ… vectorstoreãƒ‘ã‚¹ã‚’ use_openai ã«å¿œã˜ã¦è‡ªå‹•åˆ‡ã‚Šæ›¿ãˆ
    if vector_store_path is None:
        vector_store_path = get_vector_store_path(use_openai)
    else:
        vector_store_path = Path(vector_store_path)


    # âœ… ãƒ‘ã‚¹ãŒå­˜åœ¨ã—ãªã‘ã‚Œã°è©³ç´°ã‚¨ãƒ©ãƒ¼è¡¨ç¤º
    index_faiss = vector_store_path / "index.faiss"
    index_pkl = vector_store_path / "index.pkl"

    if not index_faiss.exists():
        raise FileNotFoundError(
            f"âŒ index.faiss ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {index_faiss}\n"
            f"ğŸ› ï¸ å¯¾å¿œç­–: vectorstoreæ§‹ç¯‰ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚"
        )
    if not index_pkl.exists():
        raise FileNotFoundError(
            f"âŒ index.pkl ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {index_pkl}\n"
            f"ğŸ› ï¸ å¯¾å¿œç­–: vectorstoreæ§‹ç¯‰ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚"
        )

    # ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰
    embedding_model = get_embedding_model(use_openai)
    llm = get_llm(use_openai)

    logging.info(f"ğŸ“‚ ãƒ™ã‚¯ãƒˆãƒ«DBã‚’èª­ã¿è¾¼ã¿: {vector_store_path}")
    faiss_db = FAISS.load_local(
        vector_store_path,
        embedding_model,
        allow_dangerous_deserialization=True
    )

    # ãƒã‚§ãƒ¼ãƒ³ä½œæˆã¨å®Ÿè¡Œ
    retrieval_chain = get_retrieval_chain(llm, faiss_db)

    logging.info(f"ğŸ’¬ ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›: {input_text}")
    try:
        result = retrieval_chain.invoke({"query": input_text})
    except Exception as e:
        logging.error("âŒ å›ç­”ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼:\n%s", traceback.format_exc())
        return {
            "answer": "âš ï¸ å›ç­”ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚",
            "source_documents": []
        }

    logging.info("âœ… å›ç­”ç”Ÿæˆå®Œäº†")
    return {
        "answer": result["result"],
        "source_documents": [
            {
                "page_content": doc.page_content,
                "metadata": doc.metadata
            }
            for doc in result["source_documents"]
        ]
    }

# ========== RAG ãƒãƒ£ãƒƒãƒˆã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°é–¢æ•° ========== #
def rag_chatbot_stream(input_text: str, vector_store_path: str = None, use_openai: bool = False):
    from langchain_core.prompts import PromptTemplate
    from langchain_core.runnables import RunnableMap

    # ãƒ‘ã‚¹è¨­å®š
    if vector_store_path is None:
        vector_store_path = get_vector_store_path(use_openai)
    else:
        vector_store_path = Path(vector_store_path)

    index_faiss = vector_store_path / "index.faiss"
    index_pkl = vector_store_path / "index.pkl"
    if not index_faiss.exists() or not index_pkl.exists():
        yield "âš ï¸ vectorstore ãŒå­˜åœ¨ã—ã¾ã›ã‚“ã€‚"
        return

    # ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢èª­ã¿è¾¼ã¿
    embedding_model = get_embedding_model(use_openai)
    llm = get_llm(use_openai)
    faiss_db = FAISS.load_local(vector_store_path, embedding_model, allow_dangerous_deserialization=True)
    retriever = faiss_db.as_retriever(search_kwargs={"k": 3})

    try:
        if use_openai:
            # âœ… OpenAI LLMï¼ˆStreamingå¯¾å¿œï¼‰
            from langchain_openai import ChatOpenAI
            from langchain_core.messages import HumanMessage

            docs = retriever.invoke(input_text)
            context = "\n\n".join([doc.page_content for doc in docs])
            prompt = f"""ä»¥ä¸‹ã®æ–‡æ›¸ã‚’å‚è€ƒã«ã€è³ªå•ã«ç­”ãˆã¦ãã ã•ã„ã€‚

            æ–‡æ›¸:
            {context}

            è³ªå•: {input_text}
            """

            # ChatOpenAIã¯stream=Trueã«è¨­å®š
            stream_llm = ChatOpenAI(
                model="gpt-4o",
                temperature=0,
                streaming=True,
                openai_api_key=OPENAI_API_KEY
            )

            # ğŸ”½ HumanMessage ã‚’ãƒªã‚¹ãƒˆã«ã™ã‚‹ã®ãŒé‡è¦ï¼
            for chunk in stream_llm.stream([HumanMessage(content=prompt)]):
                yield chunk.content

            yield "\n\n[END]"

        else:
            # âœ… HuggingFace LLMï¼ˆStreamingå¯¾å¿œï¼‰
            from transformers import TextStreamer
            tokenizer = llm.pipeline.tokenizer
            model = llm.pipeline.model

            docs = retriever.invoke(input_text)
            context = "\n\n".join([doc.page_content for doc in docs])
            prompt = f"""ä»¥ä¸‹ã®æ–‡æ›¸ã‚’å‚è€ƒã«ã€è³ªå•ã«ç­”ãˆã¦ãã ã•ã„ã€‚

æ–‡æ›¸:
{context}

è³ªå•: {input_text}
"""

            inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
            streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)

            import io
            from contextlib import redirect_stdout
            buffer = io.StringIO()
            with redirect_stdout(buffer):
                model.generate(**inputs, max_new_tokens=512, streamer=streamer)

            # æ–‡å­—å˜ä½ã§ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‡ºåŠ›
            for char in buffer.getvalue():
                yield char
            yield "\n\n[END]"

    except Exception as e:
        logging.error("âŒ ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼:\n%s", traceback.format_exc())
        yield f"âš ï¸ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}"


# ========== CLI å®Ÿè¡Œ ========== #
# if __name__ == "__main__":
#     query = "å–ç· å½¹ç­‰ã«é–¢ã™ã‚‹è¦å¾‹ã®è¦‹ç›´ã—ã«ã¤ã„ã¦ã©ã®ã‚ˆã†ãªã‚‚ã®ãŒã‚ã£ãŸã‹æ•™ãˆã¦ãã ã•ã„ã€‚"
#     result = rag_chatbot(query, use_openai=True)
#     print("ğŸ’¬ å›ç­”:\n", result["answer"])
#     print("ğŸ“š å‚ç…§æ–‡æ›¸:")
#     for i, doc in enumerate(result["source_documents"], 1):
#         metadata = doc.get("metadata", {})
#         print(f"{i}. {metadata.get('source', 'No Source')}")
#         print(doc["page_content"][:200], "...\n")

if __name__ == "__main__":
    query = "å–ç· å½¹ç­‰ã«é–¢ã™ã‚‹è¦å¾‹ã®è¦‹ç›´ã—ã«ã¤ã„ã¦ã©ã®ã‚ˆã†ãªã‚‚ã®ãŒã‚ã£ãŸã‹æ•™ãˆã¦ãã ã•ã„ã€‚"
    for token in rag_chatbot_stream(query, use_openai=True):  # or False
        print(token, end="", flush=True)
