import os
import logging
from typing import Union, List, Dict, Any
from pathlib import Path

import torch
from dotenv import load_dotenv

from langchain.chains import RetrievalQA
from langchain_core.documents import Document
from langchain_community.vectorstores import FAISS

# ========== è¨­å®šèª­ã¿è¾¼ã¿ ========== #
load_dotenv()

base_dir = Path(__file__).resolve().parent.parent  # src/backend ã‹ã‚‰2ã¤ä¸Šã«æˆ»ã‚‹
vector_store_path = base_dir / os.getenv("FAISS_DB_DIR")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

# ========== ãƒ­ã‚°è¨­å®š ========== #
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s"
)

# ========== ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹ç¢ºèª ========== #
device = "cuda" if torch.cuda.is_available() else "cpu"
logging.info(f"âœ… ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {device}")

# ========== ãƒ¢ãƒ‡ãƒ«å ========== #
embedding_model_name = "intfloat/multilingual-e5-small"
llm_model_name = "rinna/youri-7b-chat"

# ========== åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«é¸æŠ ========== #
def get_embedding_model(use_openai: bool):
    if use_openai:
        if not OPENAI_API_KEY:
            raise EnvironmentError("âŒ OPENAI_API_KEY ãŒæœªè¨­å®šã§ã™ã€‚")
        from langchain_openai import OpenAIEmbeddings
        logging.info("ğŸ§  OpenAI Embeddings ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
        return OpenAIEmbeddings(model="text-embedding-3-small", openai_api_key=OPENAI_API_KEY)
    else:
        from langchain_community.embeddings import HuggingFaceEmbeddings
        logging.info("ğŸ§  HuggingFace Embeddings ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
        return HuggingFaceEmbeddings(model_name=embedding_model_name)

# ========== LLM ãƒ¢ãƒ‡ãƒ«é¸æŠ ========== #
def get_llm(use_openai: bool):
    if use_openai:
        from langchain_openai import ChatOpenAI
        logging.info("ğŸ¤– OpenAI LLMï¼ˆgpt-4oï¼‰ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
        return ChatOpenAI(model="gpt-4o", temperature=0, openai_api_key=OPENAI_API_KEY)
    else:
        from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
        from langchain_community.llms import HuggingFacePipeline

        logging.info(f"ğŸ¤– HuggingFace LLMï¼ˆ{llm_model_name}ï¼‰ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...")
        tokenizer = AutoTokenizer.from_pretrained(llm_model_name)
        model = AutoModelForCausalLM.from_pretrained(llm_model_name).to(device)

        pipe = pipeline(
            "text-generation",
            model=model,
            tokenizer=tokenizer,
            device=0 if device == "cuda" else -1,
            max_new_tokens=512,
            do_sample=False
        )
        logging.info("âœ… HuggingFace LLM ã®ãƒ­ãƒ¼ãƒ‰å®Œäº†")
        return HuggingFacePipeline(pipeline=pipe)

# ========== ãƒã‚§ãƒ¼ãƒ³ç”Ÿæˆ ========== #
def get_retrieval_chain(llm, vectorstore: FAISS) -> RetrievalQA:
    return RetrievalQA.from_chain_type(
        llm=llm,
        retriever=vectorstore.as_retriever(search_kwargs={"k": 3}),
        return_source_documents=True
    )

# ========== RAG ãƒãƒ£ãƒƒãƒˆé–¢æ•° ========== #
def rag_chatbot(
    input_text: str,
    vector_store_path: str = None,
    use_openai: bool = False
) -> Dict[str, Union[str, List[Dict[str, Any]]]]:

    if vector_store_path is None:
        vector_store_path = FAISS_DB_DIR

    # ãƒ™ã‚¯ãƒˆãƒ«DBã®å­˜åœ¨ç¢ºèª
    if not (Path(vector_store_path) / "index.faiss").exists():
        raise FileNotFoundError(f"âŒ index.faiss ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {vector_store_path}")
    if not (Path(vector_store_path) / "index.pkl").exists():
        raise FileNotFoundError(f"âŒ index.pkl ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {vector_store_path}")

    # ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰
    embedding_model = get_embedding_model(use_openai)
    llm = get_llm(use_openai)

    logging.info(f"ğŸ“‚ ãƒ™ã‚¯ãƒˆãƒ«DBã‚’èª­ã¿è¾¼ã¿: {vector_store_path}")
    faiss_db = FAISS.load_local(
        vector_store_path,
        embedding_model,
        allow_dangerous_deserialization=True
    )

    # ãƒã‚§ãƒ¼ãƒ³ä½œæˆã¨å®Ÿè¡Œ
    retrieval_chain = get_retrieval_chain(llm, faiss_db)

    logging.info(f"ğŸ’¬ ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›: {input_text}")
    try:
        result = retrieval_chain.invoke({"query": input_text})
    except Exception as e:
        logging.error("âŒ å›ç­”ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼: %s", str(e))
        return {
            "answer": "âš ï¸ å›ç­”ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚",
            "source_documents": []
        }

    # å‡ºåŠ›æ•´å½¢
    logging.info("âœ… å›ç­”ç”Ÿæˆå®Œäº†")
    return {
        "answer": result["result"],
        "source_documents": [
            {
                "page_content": doc.page_content,
                "metadata": doc.metadata
            }
            for doc in result["source_documents"]
        ]
    }


# ========== CLI å®Ÿè¡Œ ========== #
if __name__ == "__main__":
    query = "ä¸æ³•è¡Œç‚ºã«ã‚ˆã‚‹æå®³è³ å„Ÿã¨ã¯ï¼Ÿ"
    result = rag_chatbot(query, use_openai=True)
    print("ğŸ’¬ å›ç­”:\n", result["answer"])
    print("ğŸ“š å‚ç…§æ–‡æ›¸:")
    for i, doc in enumerate(result["source_documents"], 1):
        metadata = doc.get("metadata", {})
        print(f"{i}. {metadata.get('source', 'No Source')}")
        print(doc["page_content"][:200], "...\n")

