import os
from pathlib import Path
from typing import List
from dotenv import load_dotenv
from docx import Document as DocxDocument
from langchain_core.documents import Document as LangchainDocument
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from tqdm import tqdm

# ============================
# .env èª­ã¿è¾¼ã¿
# ============================
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

# ============================
# åˆ‡ã‚Šæ›¿ãˆå¯èƒ½ãªåŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«
# ============================
USE_OPENAI = False  # â† OpenAI Embedding ã‚’ä½¿ã†ã‹ã©ã†ã‹

if USE_OPENAI:
    from langchain_openai import OpenAIEmbeddings
    if not OPENAI_API_KEY:
        raise ValueError("âŒ OPENAI_API_KEY ãŒ .env ã«è¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
    embedding_model = OpenAIEmbeddings(
        model="text-embedding-3-small",
        openai_api_key=OPENAI_API_KEY
    )
    print("ğŸ§  OpenAIåŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
else:
    from langchain_community.embeddings import HuggingFaceEmbeddings
    embedding_model = HuggingFaceEmbeddings(model_name="intfloat/multilingual-e5-small")
    print("ğŸ§  HuggingFaceåŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")

# ============================
# ãƒ‘ã‚¹è¨­å®š
# ============================
DOCX_DIR = Path("data/docx")
TXT_DIR = Path("data/all")
if USE_OPENAI:
    FAISS_DB_DIR = Path("vectorstore")
else:
    FAISS_DB_DIR = Path("vectorstore_hf")
# ============================
# DOCX â†’ TXT å¤‰æ›é–¢æ•°
# ============================
def convert_docx_directory(docx_dir: Path, txt_output_dir: Path):
    txt_output_dir.mkdir(parents=True, exist_ok=True)
    for docx_file in docx_dir.glob("*.docx"):
        txt_file = txt_output_dir / f"{docx_file.stem}.txt"
        if txt_file.exists():
            print(f"â­ï¸ ã‚¹ã‚­ãƒƒãƒ—: {txt_file.name}")
            continue
        try:
            doc = DocxDocument(docx_file)
            text = "\n".join(p.text for p in doc.paragraphs if p.text.strip())
            txt_file.write_text(text, encoding="utf-8")
            print(f"âœ… å¤‰æ›: {docx_file.name}")
        except Exception as e:
            print(f"âŒ ã‚¨ãƒ©ãƒ¼: {docx_file.name} - {e}")

# ============================
# TXT â†’ LangchainDocument èª­ã¿è¾¼ã¿
# ============================
def load_txt_directory(txt_dir: Path) -> List[LangchainDocument]:
    docs = []
    for file in txt_dir.glob("*.txt"):
        try:
            text = file.read_text(encoding="utf-8")
            if text.strip():
                docs.append(LangchainDocument(
                    page_content=text,
                    metadata={
                        "source": file.name,
                        "id": file.stem  # ã“ã‚Œã‚’è¿½åŠ 
                    }
                ))
                print(f"ğŸ“„ ãƒ­ãƒ¼ãƒ‰: {file.name}")
        except Exception as e:
            print(f"âš ï¸ èª­ã¿è¾¼ã¿å¤±æ•—: {file.name} - {e}")
    return docs


# ============================
# ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²ï¼ˆmetadataä»˜ãï¼‰
# ============================
def split_documents(docs: List[LangchainDocument], chunk_size=1000, chunk_overlap=100) -> List[LangchainDocument]:
    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
    chunked_docs = splitter.split_documents(docs)
    return chunked_docs

# ============================
# ãƒ™ã‚¯ãƒˆãƒ«ä¿å­˜
# ============================
def save_vector_store(docs: List[LangchainDocument], output_dir: Path, batch_size: int = 50):
    if not docs:
        print("âš ï¸ æ–‡æ›¸ãŒç©ºã§ã™")
        return

    texts = [doc.page_content for doc in docs]
    metadatas = [doc.metadata for doc in docs]
    print(f"ğŸ”¢ ãƒ™ã‚¯ãƒˆãƒ«å¤‰æ›: {len(texts)}ä»¶, æœ€é•·: {max(len(t) for t in texts)}æ–‡å­—")

    embeddings = []
    for i in tqdm(range(0, len(texts), batch_size), desc="ğŸ”„ åŸ‹ã‚è¾¼ã¿å®Ÿè¡Œä¸­"):
        batch = texts[i:i + batch_size]
        try:
            batch_vectors = embedding_model.embed_documents(batch)
            embeddings.extend(batch_vectors)
        except Exception as e:
            print(f"âŒ åŸ‹ã‚è¾¼ã¿å¤±æ•—ï¼ˆ{i}ä»¶ç›®ï¼‰: {e}")

    if not embeddings:
        print("âŒ ãƒ™ã‚¯ãƒˆãƒ«ãŒç©ºã§ã™ã€‚ä¿å­˜ã§ãã¾ã›ã‚“ã€‚")
        return

    try:
        faiss_db = FAISS.from_embeddings(
            list(zip(texts, embeddings)),
            embedding=embedding_model,
            metadatas=metadatas
        )
        output_dir.mkdir(parents=True, exist_ok=True)
        faiss_db.save_local(str(output_dir))
        print(f"âœ… ä¿å­˜å®Œäº†: {output_dir.resolve()}")
    except Exception as e:
        print(f"âŒ FAISS ä¿å­˜å¤±æ•—: {e}")

# ============================
# å®Ÿè¡Œãƒ•ãƒ­ãƒ¼
# ============================
def process_and_save():
    print(f"ğŸ“‚ DOCX â†’ TXT å¤‰æ›: {DOCX_DIR.resolve()}")
    convert_docx_directory(DOCX_DIR, TXT_DIR)

    print(f"ğŸ“ TXTèª­è¾¼: {TXT_DIR.resolve()}")
    docs = load_txt_directory(TXT_DIR)
    print(f"âœ… æ–‡æ›¸æ•°: {len(docs)}")

    print("âœ‚ï¸ ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²ä¸­...")
    chunked_docs = split_documents(docs)
    print(f"âœ… ãƒãƒ£ãƒ³ã‚¯æ•°: {len(chunked_docs)}")

    print("ğŸ’¾ ãƒ™ã‚¯ãƒˆãƒ«ä¿å­˜ä¸­...")
    save_vector_store(chunked_docs, FAISS_DB_DIR)

if __name__ == "__main__":
    process_and_save()
